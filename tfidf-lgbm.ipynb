{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":115439,"databundleVersionId":13800781,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DL-GENAi PROJECT â€” Scratch BiLSTM\n# Name  : Abhishek Saha\n# Roll  : 23f1001572\n# Model : TF-IDF + LightGBM","metadata":{}},{"cell_type":"markdown","source":"# imports","metadata":{}},{"cell_type":"code","source":"import os, re, html, time\nimport numpy as np, pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import f1_score, log_loss\nimport joblib\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport lightgbm as lgb\nimport wandb\n\nfrom scipy.sparse import hstack\n\n\n!wandb login 20d9b18a55f275c39d05bf53e51e8b328aeffff5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"class CFG:\n    TRAIN_PATH = \"/kaggle/input/2025-sep-dl-gen-ai-project/train.csv\"\n    TEST_PATH  = \"/kaggle/input/2025-sep-dl-gen-ai-project/test.csv\"\n    SAMPLE_SUB = \"/kaggle/input/2025-sep-dl-gen-ai-project/sample_submission.csv\"\n    RANDOM_SEED = 42\n    TEST_SIZE = 0.1\n\n    # TF-IDF\n    MAX_FEATURES_WORD = 40000\n    MAX_FEATURES_CHAR = 20000\n    NGRAM_RANGE_WORD = (1,2)\n    NGRAM_RANGE_CHAR = (3,5)\n\n    # LightGBM\n    LR = 0.05\n    NUM_LEAVES = 127\n    N_ESTIMATORS = 2000\n    EARLY_STOPPING_ROUNDS = 50\n\n    OUTPUT_DIR = \"./model3_outputs\"\n    WAND_PROJECT = \"23f1001572-t32025\"\n    RUN_NAME = \"model3-tfidf-lgbm\"\n\nCFG = CFG()\nos.makedirs(CFG.OUTPUT_DIR, exist_ok=True)\nTARGET_COLS = [\"anger\",\"fear\",\"joy\",\"sadness\",\"surprise\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Loader","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(CFG.TRAIN_PATH)\ntest  = pd.read_csv(CFG.TEST_PATH)\n\nprint(\"Train shape:\", train.shape)\nprint(\"Test shape :\", test.shape)\ntrain.head(2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocessing function","metadata":{}},{"cell_type":"code","source":"contraction_map = {\"n't\":\" not\", \"'re\":\" are\", \"'s\":\" is\", \"'d\":\" would\", \"'ll\":\" will\", \"'ve\":\" have\", \"'m\":\" am\"}\n\ndef preprocess_text(text):\n    if pd.isna(text):\n        return \"\"\n    s = html.unescape(str(text)).lower()\n    for k,v in contraction_map.items():\n        s = s.replace(k, v)\n    s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)\n    s = re.sub(r\"@\\w+\", \" \", s)\n    s = re.sub(r\"[^a-z0-9\\s\\.\\,\\!\\?\\']\", \" \", s)\n    s = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", s)   # coooool -> coool -> cool? keeps double\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s\n\ntrain[\"clean_text\"] = train[\"text\"].apply(preprocess_text)\ntest[\"clean_text\"]  = test[\"text\"].apply(preprocess_text)\n\n# quick sample\ntrain[\"clean_text\"].sample(3).tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train/Validation Split","metadata":{}},{"cell_type":"code","source":"train_df, val_df = train_test_split(\n    train,\n    test_size=CFG.TEST_SIZE,\n    random_state=CFG.RANDOM_SEED,\n    shuffle=True\n)\ntrain_df = train_df.reset_index(drop=True)\nval_df   = val_df.reset_index(drop=True)\n\nprint(\"Train:\", train_df.shape, \"Val:\", val_df.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## TF-IDF vectorizer","metadata":{}},{"cell_type":"code","source":"print(\"Fitting TF-IDF (word)...\")\ntfidf_word = TfidfVectorizer(max_features=CFG.MAX_FEATURES_WORD, ngram_range=CFG.NGRAM_RANGE_WORD, analyzer=\"word\")\ntfidf_word.fit(train_df[\"clean_text\"].values)\n\nprint(\"Fitting TF-IDF (char)...\")\ntfidf_char = TfidfVectorizer(max_features=CFG.MAX_FEATURES_CHAR, ngram_range=CFG.NGRAM_RANGE_CHAR, analyzer=\"char\")\ntfidf_char.fit(train_df[\"clean_text\"].values)\n\nprint(\"Transforming datasets...\")\nX_train = hstack([\n    tfidf_word.transform(train_df[\"clean_text\"].values),\n    tfidf_char.transform(train_df[\"clean_text\"].values)\n])\nX_val = hstack([\n    tfidf_word.transform(val_df[\"clean_text\"].values),\n    tfidf_char.transform(val_df[\"clean_text\"].values)\n])\nX_test = hstack([\n    tfidf_word.transform(test[\"clean_text\"].values),\n    tfidf_char.transform(test[\"clean_text\"].values)\n])\n\nprint(\"Shapes -> X_train:\", X_train.shape, \"X_val:\", X_val.shape, \"X_test:\", X_test.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"joblib.dump(tfidf_word, os.path.join(CFG.OUTPUT_DIR, \"tfidf_word.pkl\"))\njoblib.dump(tfidf_char, os.path.join(CFG.OUTPUT_DIR, \"tfidf_char.pkl\"))\nprint(\"Saved vectorizers to\", CFG.OUTPUT_DIR)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"USE_WANDB = False\ntry:\n    wandb.init(\n        project=CFG.WAND_PROJECT,\n        name=CFG.RUN_NAME,\n        config={\n            \"model\": \"tfidf_lgbm\",\n            \"max_features_word\": CFG.MAX_FEATURES_WORD,\n            \"max_features_char\": CFG.MAX_FEATURES_CHAR,\n            \"lr\": CFG.LR,\n            \"num_leaves\": CFG.NUM_LEAVES,\n            \"n_estimators\": CFG.N_ESTIMATORS\n        }\n    )\n    USE_WANDB = True\nexcept Exception as e:\n    print(\"W&B init failed or offline. Continuing without W&B. Err:\", e)\n    USE_WANDB = False\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train one LightGBM per target label","metadata":{}},{"cell_type":"code","source":"models = {}\nval_preds_proba = np.zeros((len(val_df), len(TARGET_COLS)))\ntrain_preds_proba = np.zeros((len(train_df), len(TARGET_COLS)))\n\nfor i, col in enumerate(TARGET_COLS):\n    print(f\"\\n=== Training label {col} ({i+1}/{len(TARGET_COLS)}) ===\")\n    y_train = train_df[col].values\n    y_val   = val_df[col].values\n\n    lgb_train = lgb.Dataset(X_train, label=y_train)\n    lgb_val   = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n\n    params = {\n        \"objective\": \"binary\",\n        \"metric\": \"binary_logloss\",\n        \"learning_rate\": CFG.LR,\n        \"num_leaves\": CFG.NUM_LEAVES,\n        \"verbosity\": -1,\n        \"seed\": CFG.RANDOM_SEED,\n        \"boosting_type\": \"gbdt\",\n        \"feature_pre_filter\": False\n    }\n\n    callbacks = [\n        lgb.early_stopping(CFG.EARLY_STOPPING_ROUNDS),\n        lgb.log_evaluation(100),\n    ]\n    \n    model = lgb.train(\n        params,\n        lgb_train,\n        num_boost_round=CFG.N_ESTIMATORS,\n        valid_sets=[lgb_train, lgb_val],\n        valid_names=[\"train\",\"valid\"],\n        callbacks=callbacks\n    )\n\n\n    model_fname = os.path.join(CFG.OUTPUT_DIR, f\"lgb_{col}.txt\")\n    model.save_model(model_fname)\n    models[col] = model\n\n    val_proba = model.predict(X_val, num_iteration=model.best_iteration)\n    train_proba = model.predict(X_train, num_iteration=model.best_iteration)\n    val_preds_proba[:, i] = val_proba\n    train_preds_proba[:, i] = train_proba\n\n    val_pred_bin = (val_proba > 0.5).astype(int)\n    f1 = f1_score(y_val, val_pred_bin, zero_division=0)\n    loss = log_loss(y_val, val_proba, labels=[0,1])\n\n    print(f\"Label {col} | val f1@0.5 = {f1:.4f} | val logloss = {loss:.4f}\")\n\n    if USE_WANDB:\n        wandb.log({f\"val_f1_{col}\": f1, f\"val_logloss_{col}\": loss, \"label\": col})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Threshold tuning on validation set ","metadata":{}},{"cell_type":"code","source":"best_thresholds = []\nper_label_f1 = {}\nfor i, col in enumerate(TARGET_COLS):\n    best_f1 = 0.0\n    best_t = 0.5\n    for t in np.linspace(0.1,0.9,81):\n        pbin = (val_preds_proba[:, i] > t).astype(int)\n        f1 = f1_score(val_df[col].values, pbin, zero_division=0)\n        if f1 > best_f1:\n            best_f1 = f1\n            best_t = float(t)\n    best_thresholds.append(best_t)\n    per_label_f1[col] = best_f1\n\nmacro_val_f1 = np.mean(list(per_label_f1.values()))\nprint(\"Thresholds:\", best_thresholds)\nprint(\"Per-label best F1 on validation:\", per_label_f1)\nprint(\"Macro F1 on validation:\", macro_val_f1)\n\nif USE_WANDB:\n    wandb.log({\"val_macro_f1\": macro_val_f1})\n    for i, col in enumerate(TARGET_COLS):\n        wandb.log({f\"best_thr_{col}\": best_thresholds[i], f\"best_f1_{col}\": per_label_f1[col]})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"joblib.dump({\n    \"models\": {col: os.path.join(CFG.OUTPUT_DIR, f\"lgb_{col}.txt\") for col in TARGET_COLS},\n    \"vectorizers\": {\n        \"word\": os.path.join(CFG.OUTPUT_DIR, \"tfidf_word.pkl\"),\n        \"char\": os.path.join(CFG.OUTPUT_DIR, \"tfidf_char.pkl\"),\n    },\n    \"thresholds\": best_thresholds,\n    \"config\": CFG.__dict__\n}, os.path.join(CFG.OUTPUT_DIR, \"model3_artifact.pkl\"))\n\nprint(\"Saved artifacts to:\", CFG.OUTPUT_DIR)\n\nif USE_WANDB:\n    wandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"print(\"Predicting on test set...\")\ntest_probas = np.zeros((X_test.shape[0], len(TARGET_COLS)))\nfor i, col in enumerate(TARGET_COLS):\n    model = lgb.Booster(model_file=os.path.join(CFG.OUTPUT_DIR, f\"lgb_{col}.txt\"))\n    test_proba = model.predict(X_test, num_iteration=model.best_iteration)\n    test_probas[:, i] = test_proba\n\nbest_thresholds = np.array(best_thresholds)\ntest_preds_bin = (test_probas > best_thresholds).astype(int)\n\nsubmission = pd.DataFrame({\n    \"id\": test[\"id\"],\n    \"anger\":   test_preds_bin[:, 0],\n    \"fear\":    test_preds_bin[:, 1],\n    \"joy\":     test_preds_bin[:, 2],\n    \"sadness\": test_preds_bin[:, 3],\n    \"surprise\":test_preds_bin[:, 4]\n})\nsubmission_path = os.path.join(CFG.OUTPUT_DIR, \"submission.csv\")\nsubmission.to_csv(submission_path, index=False)\nprint(\"Saved submission:\", submission_path)\nsubmission.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}